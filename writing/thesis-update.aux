\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{*}
\citation{ho2008nonnegative}
\citation{lee1999learning}
\citation{lee1999learning}
\citation{jolliffe2002principal}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{biederman1987recognition}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theories for Human Image Understanding}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cup}{{2}{5}{Theories for Human Image Understanding}{chapter.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces How does your brain take the raw retinal activations of this picture and produce a mental representation of a cup?}}{5}{figure.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Part-based recognition and visual perception}{5}{section.2.1}}
\citation{palmer1977hierarchical}
\citation{biederman1987recognition}
\citation{biederman1987recognition}
\citation{biederman1987recognition}
\citation{biederman1987recognition}
\newlabel{eq:1}{{2.1}{6}{Part-based recognition and visual perception}{equation.2.1}{}}
\citation{minsky1975framework}
\citation{minsky1975framework}
\citation{minsky1975framework}
\newlabel{fig: rbc}{{2.1}{7}{Part-based recognition and visual perception}{Item.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Processing stages for object recognition proposed by the RBC Theory. Adapted from \cite  {biederman1987recognition}}}{7}{figure.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Knowledge representation as information retrieval}{7}{section.2.2}}
\citation{biederman1987recognition}
\citation{minsky1975framework}
\citation{palmer1977hierarchical}
\citation{palmer1977hierarchical}
\citation{palmer1977hierarchical}
\citation{palmer1977hierarchical}
\citation{minsky1975framework}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The hierarchical structure of perception}{8}{section.2.3}}
\newlabel{fig:palmer}{{2.3}{8}{The hierarchical structure of perception}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The hierarchical relationship network used to represent perceptual information. At each level of the hierarchy, a structural unit (SU) is a combination of values (V) on global properties (P) and structural relationships to other structural units. Adapted from \cite  {palmer1977hierarchical}}}{8}{figure.2.3}}
\citation{biederman1987recognition}
\citation{biederman1987recognition}
\citation{biederman1987recognition}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Algorithmically formalizing cognitive mechanism}{9}{section.2.4}}
\newlabel{interp}{{2.4}{9}{Algorithmically formalizing cognitive mechanism}{section.2.4}{}}
\newlabel{manifold}{{2.4}{9}{Object Classification as Supervised Manifold Learning}{section*.2}{}}
\newlabel{fig:geonLC}{{2.4}{9}{Geons as Topics}{section*.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces An image can be represented as a linear combination of geons. Elephant adapted from \cite  {biederman1987recognition}.}}{9}{figure.2.4}}
\newlabel{tree}{{2.4}{10}{Hierarchical Organization of Topics}{section*.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The elephant from Figure \ref  {fig:geonLC} is better represented as a tree of topics.}}{10}{figure.2.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Algorithms for Representing Data}{11}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:techniques}{{3}{11}{Algorithms for Representing Data}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Singular Value Decomposition}{11}{section.3.1}}
\newlabel{section:svd}{{3.1}{11}{Singular Value Decomposition}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neural Network Representation}{13}{section.3.2}}
\newlabel{NN}{{3.2}{13}{Neural Network Representation}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: a very simple neural network with 3-dimensional input (red), one hidden layer with 4 hidden units (blue), and 2-dimensional output (green). Right: A pictorial representation of a convolutional layer of a neural network.}}{13}{figure.3.1}}
\citation{hornik1991approximation}
\citation{rumelhart1988learning}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Matrix factorization}{14}{section.3.3}}
\citation{lee1999learning}
\citation{paatero1994positive}
\citation{lee1999learning}
\newlabel{nnmf}{{3.3}{15}{Matrix factorization}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A visual representation of the non-negative matrix factorization}}{15}{figure.3.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Multiplicative Update}}{16}{algocf.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Comparison of NNMF and SVD}{16}{section.3.4}}
\newlabel{fig:svdnnmf}{{3.4}{18}{Results}{section*.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The top words for NNMF (left column in blue) and compact SVD (right column in red) factorizations of two data sets, Afghan (top) and News (bottom).}}{18}{figure.3.3}}
\citation{lee1999learning}
\citation{lee1999learning}
\citation{lee1999learning}
\citation{turk1991eigenfaces}
\citation{amodei2015deep}
\citation{hannun2014deep}
\citation{hannun2014deep}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Types of Data}{19}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Image}{19}{section.4.1}}
\newlabel{image}{{4.1}{19}{Image}{section.4.1}{}}
\citation{kawamoto2000estimation}
\citation{krause2015non}
\citation{holzapfel2008musical}
\citation{holzapfel2008musical}
\citation{ho2008nonnegative}
\newlabel{fig:face}{{4.1}{20}{Image}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A comparison of the basis vectors learned for NMF, VQ and PCA. Adapted from \cite  {lee1999learning}}}{20}{figure.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Sound}{20}{section.4.2}}
\citation{vavasis2009complexity}
\newlabel{fig:sound}{{4.2}{21}{Sound}{section.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Pairwise comparison of music genres. Adapted from \cite  {holzapfel2008musical}}}{21}{figure.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Graph}{21}{section.4.3}}
\citation{lee1999learning}
\citation{blei2012probabilistic}
\citation{blei2012probabilistic}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Natural Language}{22}{section.4.4}}
\newlabel{natlang}{{4.4}{22}{Natural Language}{section.4.4}{}}
\newlabel{fig:tmex}{{4.4}{23}{Natural Language}{section.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A visual interpretation of topic model on text. Adapted from \cite  {blei2012probabilistic}}}{23}{figure.4.3}}
\citation{cai2008non}
\citation{dicarlo2012does}
\citation{belkin2001laplacian}
\citation{cai2008non}
\citation{cai2008non}
\citation{belkin2003problems}
\citation{chung1997spectral}
\citation{chung1997spectral}
\citation{belkin2003problems}
\citation{cai2008non}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Manifold Learning as NNMF}{24}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{supervised}{{5}{24}{Manifold Learning as NNMF}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Motivation}{24}{section.5.1}}
\newlabel{O:gNNMF}{{5.1}{25}{Manifold Smoothness}{equation.5.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Multiplicative Update for Graph Regularized NNMF}}{25}{algocf.2}}
\citation{biederman1987recognition}
\citation{dicarlo2012does}
\citation{dicarlo2012does}
\citation{gross1994inferior}
\citation{dicarlo2012does}
\citation{miyashita1993inferior}
\citation{orban2008higher}
\citation{rolls2000functions}
\citation{lee2010semi}
\citation{lee2010semi}
\newlabel{manifold}{{5.1}{26}{Supervision}{section*.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A: Supervised representation of object recognition; B: seperating hyperplane via ventral stream transform. Image adapted from \cite  {dicarlo2012does}.}}{26}{figure.5.1}}
\newlabel{O:ssMMMF}{{5.3}{26}{Supervision}{equation.5.3}{}}
\newlabel{fig:semi}{{5.1}{27}{Supervision}{algocf.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces ssNNMF by label}}{27}{figure.5.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Multiplicative Update for Semi-Supervised NNMF}}{27}{algocf.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Manifold NNMF}{27}{section.5.2}}
\newlabel{mNNMF}{{5.2}{27}{Manifold NNMF}{section.5.2}{}}
\citation{nnmfimp}
\citation{nnmfimp}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Hierarchical Representation}{28}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Approach}{28}{section.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces How the tree structure is formed for the connected component vectors}}{29}{figure.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Left: Visualization of hierarchical topics in standard NMF; Right: Visualization of hierarchical topics in SSNMF; }}{30}{figure.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Visualization}{30}{section.6.2}}
\citation{mhaskar2016learning}
\citation{mhaskar2016learning}
\citation{mhaskar2016learning}
\citation{mhaskar2016learning}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Topics used in hierarchical topic model}}{31}{figure.6.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Deep Models}{31}{section.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Motivation}{31}{subsection.6.3.1}}
\citation{trigeorgis2014deep}
\citation{trigeorgis2014deep}
\citation{trigeorgis2014deep}
\citation{trigeorgis2014deep}
\citation{deepNonNeg}
\newlabel{poggio}{{6.3.1}{32}{Motivation}{subsection.6.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces A shallow versus deep neural network. The number of parameters a shallow network estimates is exponential in the number of inputs, whereas the number of parameters a deep model estimates is linear in the number of inputs. Image adapted from \cite  {mhaskar2016learning}.}}{32}{figure.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}A Deep NNMF Model}{32}{subsection.6.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Deep Neural Models}{32}{subsection.6.3.3}}
\newlabel{deepNNMF}{{6.3.2}{33}{A Deep NNMF Model}{subsection.6.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Recursive factoring results in a hierarchy of attributes. Image adapted from \cite  {trigeorgis2014deep}.}}{33}{figure.6.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Discussion}{34}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{20news}
\bibstyle{plain}
\bibdata{myBib}
\bibcite{amodei2015deep}{1}
\bibcite{belkin2003problems}{2}
\bibcite{belkin2001laplacian}{3}
\bibcite{biederman1987recognition}{4}
\bibcite{blei2012probabilistic}{5}
\bibcite{cai2008non}{6}
\bibcite{chung1997spectral}{7}
\bibcite{dicarlo2012does}{8}
\bibcite{deepNonNeg}{9}
\bibcite{fortunato2010community}{10}
\bibcite{griffiths2004hierarchical}{11}
\bibcite{gross1994inferior}{12}
\bibcite{hannun2014deep}{13}
\bibcite{ho2008nonnegative}{14}
\bibcite{holzapfel2008musical}{15}
\bibcite{hornik1991approximation}{16}
\bibcite{jolliffe2002principal}{17}
\bibcite{kawamoto2000estimation}{18}
\bibcite{krause2015non}{19}
\bibcite{lancichinetti2009detecting}{20}
\bibcite{lee1997unsupervised}{21}
\bibcite{lee1999learning}{22}
\bibcite{lee2010semi}{23}
\bibcite{logothetis1996visual}{24}
\bibcite{mhaskar2016learning}{25}
\bibcite{minsky1975framework}{26}
\bibcite{miyashita1993inferior}{27}
\bibcite{orban2008higher}{28}
\bibcite{paatero1994positive}{29}
\bibcite{palmer1977hierarchical}{30}
\bibcite{20news}{31}
\bibcite{rolls2000functions}{32}
\bibcite{rumelhart1988learning}{33}
\bibcite{nnmfimp}{34}
\bibcite{trigeorgis2014deep}{35}
\bibcite{turk1991eigenfaces}{36}
\bibcite{ullman1996high}{37}
\bibcite{vavasis2009complexity}{38}
\bibcite{wachsmuth1994recognition}{39}
